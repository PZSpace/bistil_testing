====================================================================================================
MODEL COMPARISON RESULTS
====================================================================================================

MODEL INFORMATION
----------------------------------------------------------------------------------------------------
Model                                    Parameters      Vocab      Layers   Hidden  
----------------------------------------------------------------------------------------------------
mbert-en-ibo-trimmed-vocab               108,152,399     28,751     12       768     
bert-base-multilingual-cased             177,974,523     119,547    12       768     
mbert-en-ibo-6-layers-trimmed-vocab      65,625,167      28,751     6        768     

SOURCE LANGUAGE PERFORMANCE
----------------------------------------------------------------------------------------------------
Model                                    Loss         Perplexity      Samples   
----------------------------------------------------------------------------------------------------
mbert-en-ibo-trimmed-vocab               1.9140       6.7804          8,383     
bert-base-multilingual-cased             2.2706       9.6852          8,359     
mbert-en-ibo-6-layers-trimmed-vocab      10.3004      29,744.2524     8,383     

TARGET LANGUAGE PERFORMANCE
----------------------------------------------------------------------------------------------------
Model                                    Loss         Perplexity      Samples   
----------------------------------------------------------------------------------------------------
mbert-en-ibo-trimmed-vocab               3.5673       35.4191         4,005     
bert-base-multilingual-cased             3.8064       44.9887         4,001     
mbert-en-ibo-6-layers-trimmed-vocab      10.2605      28,580.8538     4,005     

AVERAGE PERFORMANCE (Source + Target)
----------------------------------------------------------------------------------------------------
Model                                    Avg Loss     Avg Perplexity  Rank      
----------------------------------------------------------------------------------------------------
mbert-en-ibo-trimmed-vocab               2.7406       21.0997         #1 ‚≠ê BEST
bert-base-multilingual-cased             3.0385       27.3369         #2
mbert-en-ibo-6-layers-trimmed-vocab      10.2804      29,162.5531     #3
====================================================================================================

SUMMARY
----------------------------------------------------------------------------------------------------
Best Model: mbert-en-ibo-trimmed-vocab
  Average Perplexity: 21.0997
  Parameters: 108,152,399

Improvement over worst model: 99.93% lower perplexity
Parameter reduction: -64.80% fewer parameters
====================================================================================================